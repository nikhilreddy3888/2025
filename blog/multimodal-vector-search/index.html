<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"November 20, 2024"),l="Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings",r="In an era where documents seamlessly blend rich textual and visual content, traditional retrieval systems often fall short. This post explores how Multimodal Embeddings bridge the modality gap, introducing advanced models such as VISTA and ColPali, along with innovative evaluation methods, enhanced model interpretability, and an analysis of their performance.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+l.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${l}},\n  abstract = {${r}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${l}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="In an era where documents seamlessly blend rich textual and visual content, traditional retrieval systems often fall short. This post explores how Multimodal Embeddings bridge the modality gap, introducing advanced models such as VISTA and ColPali, along with innovative evaluation methods, enhanced model interpretability, and an analysis of their performance."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://nikhilreddy3888.github.io/2025/blog/multimodal-vector-search/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">{
      "title": "Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings",
      "description": "In an era where documents seamlessly blend rich textual and visual content, traditional retrieval systems often fall short. This post explores how Multimodal Embeddings bridge the modality gap, introducing advanced models such as VISTA and ColPali, along with innovative evaluation methods, enhanced model interpretability, and an analysis of their performance.",
      "published": "November 20, 2024",
      "authors": [
        {
          "author": "Nikhil Reddy",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Mila, Quebec",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings</h1> <p>In an era where documents seamlessly blend rich textual and visual content, traditional retrieval systems often fall short. This post explores how Multimodal Embeddings bridge the modality gap, introducing advanced models such as VISTA and ColPali, along with innovative evaluation methods, enhanced model interpretability, and an analysis of their performance.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#challenges-in-document-retrieval">Challenges in Document Retrieval</a></div> <ul> <li><a href="#modality-specific-challenges">Modality-Specific Challenges</a></li> <li><a href="#modality-gap">Modality Gap</a></li> </ul> <div><a href="#limitations-of-traditional-models">Limitations of Traditional Models</a></div> <ul> <li><a href="#text-based-models">Text-Based Models</a></li> <li><a href="#vision-based-models">Vision-Based Models</a></li> </ul> <div><a href="#advanced-evaluation-metrics-for-ranking">Advanced Evaluation Metrics for Ranking</a></div> <ul> <li><a href="#mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</a></li> <li><a href="#normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (nDCG)</a></li> <li><a href="#mean-average-precision-map">Mean Average Precision (MAP)</a></li> </ul> <div><a href="#recent-advancements-in-multimodal-retrieval-models">Recent Advancements in Multimodal Retrieval Models</a></div> <ul> <li><a href="#vista-vision-augmented-text-embeddings">VISTA: Vision-Augmented Text Embeddings</a></li> <li><a href="#colpali-efficient-document-retrieval-with-vision-language-models">ColPali: Efficient Document Retrieval with Vision Language Models</a></li> </ul> <div><a href="#interpretability-of-advance-models-in-documents">Interpretability of Advance Models in Documents</a></div> <ul> <li><a href="#importance-of-interpretability">Importance of Interpretability</a></li> <li><a href="#generating-heatmaps-using-attention-mechanisms">Generating Heatmaps Using Attention Mechanisms</a></li> <li><a href="#heatmap-insights-case-studies">Heatmap Insights: Case Studies</a></li> </ul> <div><a href="#innovative-evaluation-methods">Innovative Evaluation Methods</a></div> <ul> <li><a href="#feature-search-experiments">Feature Search Experiments</a></li> <li><a href="#feature-similarity-experiments">Feature Similarity Experiments</a></li> <li><a href="#interactive-embedding-visualization">Interactive Embedding Visualization</a></li> </ul> <div><a href="#experimental-evaluations">Experimental Evaluations</a></div> <ul> <li><a href="#performance-comparison">Performance Comparison</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <ul> <li><a href="#key-takeaways">Key Takeaways</a></li> </ul> </nav> </d-contents> <p>In today’s data-driven world, the volume and complexity of information have grown exponentially. Documents are no longer confined to plain text; they now encompass a rich blend of images, charts, tables, and intricate layouts. This evolution presents a significant challenge: <strong>How do we effectively retrieve and analyze information from such complex, multimodal documents?</strong> <br> Traditional retrieval systems, primarily designed for text-only data, often falter when faced with this complexity. They struggle to extract and interpret the valuable information embedded within various modalities. This limitation hampers our ability to harness the full potential of the data at our disposal. <br> Enter <strong>Multimodal Embeddings</strong>—an innovative approach that leverages both textual and visual data to revolutionize document retrieval. By bridging the <strong>modality gap</strong> between different data types, MVS(MultiModal Vector Search) promises to make information retrieval more accurate and efficient than ever before. <br> In this blog post, we’ll delve into:</p> <ul> <li>The unique challenges that modern, complex documents pose for retrieval tasks.</li> <li>The limitations of traditional text-only and vision-only models.</li> <li>Understanding the <strong>modality gap</strong> and its impact on multimodal retrieval.</li> <li>Exploring cutting-edge multimodal models like <strong>VISTA</strong> <d-cite key="Zhou2024VISTA"></d-cite> and <strong>ColPali</strong> <d-cite key="Faysse2024ColPali"></d-cite>.</li> <li>Investigating the interpretability, with empirical results showcasing model performance.</li> <li>Novel benchmarking strategies designed to evaluate these advanced models effectively.</li> </ul> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 1: Integration of text and visual modalities in document retrieval. <d-cite key="ma2024unifyingmultimodalretrievaldocument"></d-cite> </div> <p>With the rise of sophisticated document formats that integrate text, images, and complex layouts has rendered traditional text-based retrieval systems inadequate. The richness of multimodal documents requires systems that can understand and process multiple data types simultaneously. The <strong>modality gap</strong>—the disconnect between different types of data representations—poses a significant hurdle in achieving effective retrieval<d-cite key="Liang2022MindGap"></d-cite>.</p> <p>To bridge this gap, advanced multimodal systems are essential. By aligning and embedding various data types, these systems not only interpret complex documents but also open doors to more powerful, nuanced information retrieval across diverse formats.</p> <h2 id="challenges-in-document-retrieval">Challenges in Document Retrieval</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/main.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 2: Illustration of the increasing levels of detail in images, from basic visuals to more information-dense representations. </div> <h3 id="modality-specific-challenges">Modality-Specific Challenges</h3> <p>Retrieving information from today’s complex documents is a multifaceted problem that requires a nuanced approach. Documents now often contain:</p> <ul> <li> <strong>Text</strong>: Dense paragraphs, bullet points, and annotations.</li> <li> <strong>Images</strong>: Photographs, diagrams, and illustrations.</li> <li> <strong>Tables and Charts</strong>: Structured data representations.</li> <li> <strong>Complex Layouts</strong>: Multi-column formats, sidebars, and embedded multimedia.</li> </ul> <p>Each modality presents unique technical obstacles:</p> <ol> <li> <strong>Textual Data</strong>: <ul> <li> <strong>Language Ambiguity</strong>: Synonyms, homonyms, and context-dependent meanings.</li> <li> <strong>Multilingual Content</strong>: Documents may contain multiple languages or dialects.</li> </ul> </li> <li> <strong>Visual Data</strong>: <ul> <li> <strong>Image Quality</strong>: Low-resolution images can hinder recognition.</li> <li> <strong>Complex Visuals</strong>: Diagrams and charts may contain dense information that’s hard to parse.</li> </ul> </li> <li> <strong>Structural Layout</strong>: <ul> <li> <strong>Non-linear Reading Paths</strong>: Multi-column texts and inserts can confuse linear text processors.</li> <li> <strong>Embedded Elements</strong>: Images and tables interwoven with text complicate parsing.</li> </ul> </li> </ol> <p>Addressing these challenges requires advanced retrieval systems that are capable of seamlessly integrating and processing each modality, making it possible to extract meaningful insights from even the most complex document formats.</p> <h3 id="modality-gap">Modality Gap</h3> <p>The <strong>modality gap</strong> refers to the disconnect between text and image embeddings in multimodal models. Despite shared semantic objectives, these embeddings often occupy distinct regions in the space, making it difficult for models to relate information across modalities<d-cite key="Liang2022MindGap"></d-cite>.</p> <h4 id="causes-of-the-gap">Causes of the Gap</h4> <ol> <li> <strong>Separate Embedding Spaces</strong>: Text and images are encoded differently, forming distinct clusters.</li> <li> <strong>Contrastive Learning Bias</strong>: Training inadvertently emphasizes modality-specific features.</li> <li> <strong>Initialization Bias</strong>: Pretrained encoders begin with cone-shaped distributions, reinforcing separation.</li> </ol> <h4 id="evidence-from-flickr8k-dataset">Evidence from Flickr8k Dataset</h4> <p>Using 1,000 text-image pairs from the Flickr8k test set, each pair consists of a single image and five caption texts, all describing the same image. Embeddings were generated for both the texts and the image, and cosine similarity was calculated and visualized.</p> <ul> <li> <strong>Cosine Similarity</strong>: Text-text pairs had higher similarity than image-text pairs, despite semantic overlap.</li> <li> <strong>Visualization</strong>: Embeddings from the Flickr8k dataset showed clustering into distinct text and image regions.</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 3: Visualization of the modality gap between text and image embeddings. </div> <h4 id="implications">Implications</h4> <ul> <li> <strong>Retrieval Challenges</strong>: Models may retrieve documents that match in one modality but are irrelevant in another.</li> <li> <strong>Increased Complexity</strong>: Bridging this gap demands advanced architectures and computational resources.</li> </ul> <p>For more details on the Modality Gap, refer to <a href="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/" rel="external nofollow noopener noopener noreferrer" target="_blank">this article</a>.</p> <h2 id="limitations-of-traditional-models">Limitations of Traditional Models</h2> <h3 id="text-based-models">Text-Based Models</h3> <p>Text-based retrieval models—leveraging techniques like TF-IDF, BM25, and transformer-based embeddings such as BERT—have been the cornerstone of information retrieval. They excel at understanding and retrieving information when text is the primary medium. <d-cite key="devlin2019bertpretrainingdeepbidirectional"></d-cite> <d-cite key="reimers2019sentencebertsentenceembeddingsusing"></d-cite></p> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Blind to Visual Content</strong>: Unable to interpret images, charts, or layouts.</li> <li> <strong>Ignoring Spatial Relationships</strong>: Can’t understand the importance of where text appears in a document.</li> <li> <strong>Struggling with Non-Linear Layouts</strong>: Fail to process documents with complex formatting.</li> </ul> <h3 id="vision-based-models">Vision-Based Models</h3> <p>Vision-based retrieval models, utilizing architectures like convolutional neural networks (CNNs) or vision transformers (e.g., ViT, Swin Transformer), extract features from visual content, focusing on images, diagrams, and spatial layouts. <d-cite key="he2015deepresiduallearningimage"></d-cite> <d-cite key="liu2021swintransformerhierarchicalvision"></d-cite></p> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Text Interpretation</strong>: Struggle with reading or understanding embedded text within images.</li> <li> <strong>Fine-Grained Details</strong>: May overlook small fonts or intricate details in complex diagrams.</li> <li> <strong>Semantic Gap</strong>: Lack understanding of the textual semantics associated with visual elements.</li> </ul> <h2 id="advanced-evaluation-metrics-for-ranking">Advanced Evaluation Metrics for Ranking</h2> <p>Evaluating retrieval systems, especially those handling multimodal data, demands metrics that account for both relevance and ranking position. <d-cite key="arabzadeh2024comparisonmethodsevaluatinggenerative"></d-cite> <d-cite key="jadon2024comprehensivesurveyevaluationtechniques"></d-cite></p> <h3 id="mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</h3> <p><strong>Definition:</strong> The MRR measures how quickly a system retrieves the first relevant document.</p> \[\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}\] <p>Where:</p> <ul> <li>\(( Q )\) is the number of queries.</li> <li>\(( {rank}_i )\) is the rank position of the first relevant document for the ( i )-th query.</li> </ul> <p><strong>Importance:</strong></p> <ul> <li> <strong>User Satisfaction</strong>: Higher MRR indicates users find relevant documents sooner.</li> <li> <strong>System Efficiency</strong>: Reflects the system’s ability to prioritize relevant results.</li> </ul> <h3 id="normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (nDCG)</h3> <p><strong>Definition:</strong> nDCG evaluates ranking quality by considering the position of relevant documents and assigning higher importance to top-ranked results.</p> \[\text{nDCG}_p = \frac{1}{\text{IDCG}_p} \sum_{i=1}^p \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}\] <p>Where:</p> <ul> <li>\(( p )\) is the number of results considered.</li> <li>\(( {rel}_i )\) is the relevance score of the result at position ( i ).</li> <li>\(( {IDCG}_p )\) is the ideal DCG up to position ( p ).</li> </ul> <p><strong>Importance:</strong></p> <ul> <li> <strong>Relevance and Rank</strong>: Balances both factors to provide a holistic evaluation.</li> <li> <strong>Comparison Across Queries</strong>: Normalization allows for fair comparison between different queries.</li> </ul> <h3 id="mean-average-precision-map">Mean Average Precision (MAP)</h3> <p><strong>Definition:</strong> MAP computes the mean of average precision scores across all queries.</p> \[\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{n_q} \sum_{k=1}^{n_q} \text{Precision}(k) \times \text{rel}(k)\] <p>Where:</p> <ul> <li>\(( {n}_q )\) is the number of retrieved documents for query ( q ).</li> <li>\(( {Precision}(k) )\) is the precision at cutoff ( k ).</li> <li>\(( {rel}(k) )\) is a binary indicator of relevance at position ( k ).</li> </ul> <p><strong>Importance:</strong></p> <ul> <li> <strong>Comprehensive Evaluation</strong>: Accounts for all relevant documents, not just the first.</li> <li> <strong>Balanced Metric</strong>: Reflects both precision and recall across the ranking.</li> </ul> <h2 id="recent-advancements-in-multimodal-retrieval-models">Recent Advancements in Multimodal Retrieval Models</h2> <h3 id="vista-vision-augmented-text-embeddings">VISTA: Vision-Augmented Text Embeddings</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/vista.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 3: VISTA's architecture integrates visual tokens into text embeddings.<d-cite key="Zhou2024VISTA"></d-cite> </div> <p><strong>Overview:</strong></p> <p>VISTA (Visualized Text Embedding For Universal Multi-Modal Retrieval) is a model that aims to enhance text embeddings with visual information, effectively bridging the modality gap.</p> <p><strong>How VISTA Works:</strong></p> <ul> <li> <strong>Visual Token Embedding</strong>: Introduces visual tokens derived from document images into the text embedding space.</li> <li> <strong>Extension of Text Encoders</strong>: Enhances pre-trained text models (like BERT) by adding a visual component.</li> <li> <strong>Multi-Stage Training</strong>: <ul> <li> <strong>Alignment Phase</strong>: Aligns visual tokens with textual tokens using a large corpus.</li> <li> <strong>Fine-Tuning Phase</strong>: Trains on composed image-text data for multimodal representation capability.</li> </ul> </li> </ul> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Improved Retrieval Accuracy</strong>: Enhances understanding of documents with embedded images.</li> <li> <strong>Compatibility</strong>: Works with existing text encoders.</li> <li> <strong>Efficiency</strong>: Avoids the need for extensive retraining.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Dependency on Visual Quality</strong>: Performance may degrade with low-quality images.</li> <li> <strong>Complexity in Token Integration</strong>: Requires careful balancing to prevent one modality from dominating.</li> </ul> <h3 id="colpali-efficient-document-retrieval-with-vision-language-models">ColPali: Efficient Document Retrieval with Vision Language Models</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 4: ColPali's unified embedding space for text and images.<d-cite key="Faysse2024ColPali"></d-cite> </div> <p><strong>Overview:</strong></p> <p>ColPali is a state-of-the-art multimodal retrieval model that leverages Vision-Language Models (VLMs) to create a unified embedding space for text and images.</p> <p><strong>How ColPali Works:</strong></p> <ul> <li> <strong>Vision-Language Integration</strong>: Uses a dual-encoder architecture with separate encoders for text and images.</li> <li> <strong>Late Interaction Mechanism</strong>: Retains individual token embeddings for richer interactions.</li> <li> <strong>Elimination of OCR Bottleneck</strong>: Processes images directly, capturing both textual and visual information without OCR.</li> </ul> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Effective Modality Bridging</strong>: Creates a shared space for all modalities.</li> <li> <strong>Enhanced Retrieval Performance</strong>: Excels in retrieving documents with complex layouts.</li> <li> <strong>Interpretability</strong>: Allows for analyzing which tokens contribute most to the retrieval score.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Computational Demands</strong>: Training VLMs is resource-intensive.</li> <li> <strong>Data Requirements</strong>: Requires large amounts of multimodal data.</li> <li> <strong>Potential Overfitting</strong>: May overfit to specific layouts or styles.</li> </ul> <h2 id="interpretability-of-advanced-models-in-documents">Interpretability of Advanced Models in Documents</h2> <h3 id="importance-of-interpretability">Importance of Interpretability</h3> <p>Understanding how a model like ColPali makes retrieval decisions is crucial, particularly in sensitive domains like finance, where accuracy and accountability are paramount.</p> <ul> <li> <strong>Transparency</strong>: By identifying the specific document regions influencing retrieval, ColPali provides clear insights into its decision-making process, fostering user trust.</li> <li> <strong>Debugging</strong>: The ability to pinpoint errors allows developers to refine and optimize the model effectively.</li> </ul> <h3 id="generating-heatmaps-using-attention-mechanisms">Generating Heatmaps Using Attention Mechanisms</h3> <p>Models leverages attention mechanisms to produce interpretable heatmaps that highlight the most relevant regions of a document in response to a query. By computing attention scores between query tokens and image patches, the model identifies which parts of the document image are most influential in the retrieval process.</p> <p><strong>How Heatmaps are Generated:</strong></p> <ol> <li> <strong>Image Patches and Query Tokens</strong>: The document image is divided into fixed-size patches, and the query is broken down into individual tokens.</li> <li> <strong>Embedding Computation</strong>: The model computes embeddings for both the image patches and the query tokens using its vision and language encoders.</li> <li> <strong>Attention Score Calculation</strong>: Attention scores are calculated by taking the dot product between each query token embedding and each image patch embedding.</li> <li> <strong>Normalization and Mapping</strong>: These scores are normalized to highlight the most significant interactions and are mapped back onto the spatial layout of the image patches.</li> <li> <strong>Visualization</strong>: The normalized attention scores are overlaid onto the original document image, creating heatmaps that visually represent the areas of focus for each query token.</li> </ol> <p>This approach provides a transparent way to understand which parts of the document are most relevant to the query, combining both textual and visual information.</p> <h3 id="heatmap-insights-case-studies">Heatmap Insights: Case Studies</h3> <p><strong>Example 1: Alibaba’s 10-K Report</strong></p> <ul> <li> <strong>Query</strong>: “What are the artificial intelligence tools being developed?”</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 5: Heatmaps overlaid on Alibaba's 10-K report highlighting "artificial" and "intelligence". </div> <ul> <li> <strong>Results</strong>: ColPali retrieved relevant sections emphasizing AI tools and infrastructure, such as “AI-driven infrastructure” and “intelligent monitoring.”</li> <li> <strong>Explanation</strong>: The heatmaps show that the model’s attention is concentrated on areas containing the terms “artificial” and “intelligence,” as well as related phrases like “AI” and “intelligent.” This indicates that ColPali effectively aligns query tokens with corresponding regions in the document image, highlighting its ability to interpret both textual content and visual context.</li> </ul> <p><strong>Example 2: Royal Bank of Canada’s Annual Report</strong></p> <ul> <li> <strong>Query</strong>: “Describe the elements of stock-based compensation.”</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 6: Heatmaps highlighting relevant terms "stock" and "compensation" in RBC's annual report. </div> <ul> <li> <strong>Results</strong>: Pages discussing “stock-based compensation plans” and related financial methodologies were retrieved.</li> <li> <strong>Explanation</strong>: The heatmaps reveal that the model focuses on sections containing the words “stock,” “compensation,” and related terms like “share-based” and “shareholders.” Even when the text appears in smaller fonts or footnotes, the model effectively identifies these areas. This demonstrates ColPali’s capability to extract precise financial details from complex document layouts by leveraging attention mechanisms.</li> </ul> <p>These examples showcase how ColPali’s use of attention-based heatmaps provides interpretable insights into its retrieval decisions, highlighting the relevance between query tokens and document regions.</p> <p>For more details on the interpretability and the underlying code, refer to <a href="https://medium.com/@hlealpablo/interpretability-of-colpali-in-financial-documents-5a2dcdeeba3a" rel="external nofollow noopener noopener noreferrer" target="_blank">this article</a>.</p> <h2 id="innovative-evaluation-methods-simple-yet-effective">Innovative Evaluation Methods (Simple yet Effective)</h2> <h3 id="feature-search-experiments">Feature Search Experiments</h3> <p><strong>Objective:</strong></p> <p>Assess how effectively models generate embeddings that capture similarities between documents across modalities.</p> <p><strong>Methodology:</strong></p> <ol> <li> <strong>Embedding Extraction</strong>: Used models like <strong>BGE-M3</strong>, <strong>VISTA</strong>, <strong>ColPali</strong>, and <strong>ColQwen</strong>.</li> <li> <strong>Similarity Computation</strong>: Calculated cosine similarity between document embeddings.</li> <li> <strong>Evaluation Metrics</strong>: Employed metrics such as <strong>Precision@K</strong>, <strong>Recall@K</strong>, <strong>MRR</strong>, <strong>MAP</strong>, and <strong>nDCG</strong>.</li> </ol> <p><strong>Results:</strong></p> <ul> <li> <strong>Text-Based Models</strong>: High precision when text is the differentiator but struggled with visual nuances.</li> <li> <strong>Multimodal Models</strong>: Outperformed in tasks where visual context is important, effectively capturing both textual and visual similarities.</li> </ul> <h3 id="feature-similarity-experiments">Feature Similarity Experiments</h3> <p><strong>Objective:</strong></p> <p>Evaluate the usefulness of embeddings in differentiating between various document types.</p> <p><strong>Methodology:</strong></p> <ol> <li> <strong>Prototype Vectors Generation</strong>: Created prototype vectors for each document category (e.g., invoices, contracts, reports) representing the “center” of the embedding space for that class.</li> <li> <strong>Similarity with Cluster Centers</strong>: Compared new document embeddings to these prototype vectors using cosine similarity, classifying each document into the category with the highest similarity score.</li> <li> <strong>Evaluation Metrics</strong>: Used metrics such as <strong>Accuracy</strong> and <strong>Recall</strong> to assess how well the embeddings captured similarities within categories and differences between them.</li> </ol> <p><strong>Findings:</strong></p> <ul> <li> <strong>Text-Based Models</strong>: Effectively categorized documents with distinct textual content.</li> <li> <strong>Multimodal Models</strong>: Performed better when visual and layout information were crucial for classification, highlighting their ability to capture complex document features.</li> </ul> <h3 id="interactive-embedding-visualization">Interactive Embedding Visualization</h3> <p>Below is an interactive t-SNE plot showing document embeddings colored by category. This visualization provides insights into how different models (e.g., VISTA, ColPali, SigLIP) represent document categories (e.g., forms, invoices, identity documents). The embeddings are displayed in a two-dimensional space, highlighting clustering patterns and category separations.</p> <div class="row mt-3"> <iframe src="/2025/assets/html/2025-04-28-multimodal-vector-search/final_combined_embeddings.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Figure 7: Interactive t-SNE plot of document embeddings colored by category. </div> <p>This additional visualization allows for a deeper understanding of how well multimodal models differentiate between document types, bridging the modality gap through effective embedding representations.</p> <h2 id="experimental-evaluations">Experimental Evaluations</h2> <p>To evaluate the performance of multimodal retrieval models, we utilized the <strong>ViDoRe Benchmark</strong>, a comprehensive collection designed for assessing document retrieval using visual features. This benchmark includes datasets formatted in a Question-Answering (QA) style to simulate realistic retrieval scenarios.</p> <p>These datasets encompass a wide range of document types, including financial reports, legal documents, academic papers, manuals, and healthcare records. Each dataset presents unique challenges due to varying content complexity, layouts, and modality combinations.</p> <h3 id="performance-comparison">Performance Comparison:</h3> <p>The table below summarizes the performance of various models across the ViDoRe datasets, measured by the Normalized Discounted Cumulative Gain at rank 5 (NDCG@5).</p> <table> <thead> <tr> <th><strong>Model Name</strong></th> <th><strong>Average</strong></th> <th><strong>TAT-DQA</strong></th> <th><strong>Shift Project</strong></th> <th><strong>Artificial Intelligence</strong></th> <th><strong>Government Reports</strong></th> <th><strong>ArxivQA</strong></th> <th><strong>DocVQA</strong></th> <th><strong>Healthcare Industry</strong></th> <th><strong>InfoVQA</strong></th> <th><strong>Energy</strong></th> <th><strong>TabFQuad</strong></th> </tr> </thead> <tbody> <tr> <td><strong>ColQwen2</strong></td> <td><strong>89.3</strong></td> <td>81.4</td> <td>90.7</td> <td>99.4</td> <td>96.3</td> <td>88.1</td> <td>60.6</td> <td>98.1</td> <td>92.6</td> <td>95.9</td> <td>89.5</td> </tr> <tr> <td> <strong>ColPali</strong><d-cite key="Faysse2024ColPali"></d-cite> </td> <td>81.3</td> <td>65.8</td> <td>73.2</td> <td>96.2</td> <td>92.7</td> <td>79.1</td> <td>54.4</td> <td>94.4</td> <td>81.8</td> <td>91.0</td> <td>83.9</td> </tr> <tr> <td> <strong>VISTA</strong>*<d-cite key="Zhou2024VISTA"></d-cite> </td> <td>70.8</td> <td>56.9</td> <td>78.6</td> <td>86.8</td> <td>89.3</td> <td>39.4</td> <td>32.2</td> <td>91.1</td> <td>75.0</td> <td>87.7</td> <td>71.2</td> </tr> <tr> <td> <strong>E5-Large</strong>*<d-cite key="Wang2022E5"></d-cite> </td> <td>65.0</td> <td>51.0</td> <td>61.1</td> <td>87.9</td> <td>84.8</td> <td>34.0</td> <td>27.8</td> <td>85.5</td> <td>63.5</td> <td>81.6</td> <td>73.1</td> </tr> <tr> <td> <strong>BGE-M3</strong>*<d-cite key="Xiao2023BGE"></d-cite> </td> <td>67.0</td> <td>43.8</td> <td>73.1</td> <td>88.8</td> <td>80.4</td> <td>35.7</td> <td>32.9</td> <td>91.3</td> <td>71.9</td> <td>83.3</td> <td>69.1</td> </tr> <tr> <td><strong>BM25</strong></td> <td>65.5</td> <td>62.7</td> <td>64.3</td> <td>92.8</td> <td>83.9</td> <td>31.6</td> <td>36.8</td> <td>87.2</td> <td>62.9</td> <td>85.9</td> <td>46.5</td> </tr> <tr> <td> <strong>SigLIP</strong><d-cite key="zhai2023sigmoid"></d-cite> </td> <td>51.4</td> <td>26.2</td> <td>18.7</td> <td>62.5</td> <td>66.1</td> <td>43.2</td> <td>30.3</td> <td>79.1</td> <td>64.1</td> <td>65.7</td> <td>58.1</td> </tr> <tr> <td> <strong>Jina-CLIP</strong><d-cite key="koukounas2024jina"></d-cite> </td> <td>17.7</td> <td>3.3</td> <td>3.8</td> <td>15.2</td> <td>21.4</td> <td>25.4</td> <td>11.9</td> <td>20.8</td> <td>35.5</td> <td>19.7</td> <td>20.2</td> </tr> </tbody> </table> <p><strong>Note</strong>: Models marked with <code class="language-plaintext highlighter-rouge">*</code> (e.g., VISTA, E5-Large, BGE-M3) have been re-evaluated on the ViDoRe Benchmark.</p> <p><strong>Observations:</strong></p> <ul> <li> <p><strong>Top Performer:</strong> The <strong>ColQwen2</strong> model achieved the highest average NDCG@5 score of <strong>89.3</strong>, outperforming other models across most datasets.</p> </li> <li> <p><strong>Strong Multimodal Performance:</strong> <strong>ColQwen2</strong>, <strong>VISTA</strong> and <strong>ColPali</strong> performed well across multiple datasets, showing robust results on specific domain datasets like <strong>Healthcare Industry</strong> and <strong>Artificial Intelligence</strong>.</p> </li> <li> <p><strong>Text vs. Vision Models:</strong> Traditional text-based models such as <strong>BGE-M3</strong> and <strong>BM25</strong> showed competitive results on certain datasets but generally lagged behind multimodal models. Vision-only models like <strong>SigLIP</strong> and <strong>Jina-CLIP</strong> struggled significantly, especially on text-heavy datasets.</p> </li> <li> <p><strong>Dataset Variability:</strong> The performance variance across datasets indicates that some models are better suited for specific domains. For instance, <strong>ColQwen2</strong> excelled in <strong>Government Reports</strong> (96.3) and <strong>Energy</strong> (95.9), suggesting robustness in handling domain-specific jargon and layouts.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The shift from unimodal to multimodal approaches in document retrieval is revolutionizing how we access complex information. Models like <strong>VISTA</strong>, <strong>ColPali</strong>, and <strong>ColQwen2</strong> not only bridge the modality gap but also set new benchmarks for performance across diverse and complex datasets.</p> <h3 id="key-takeaways">Key Takeaways:</h3> <ol> <li> <strong>Multimodal Models Excel</strong>: Combining textual and visual features significantly improves retrieval accuracy, especially for documents with complex layouts.</li> <li> <strong>Advanced Models Address Modality Challenges</strong>: Models like ColPali and ColQwen2 create unified embedding spaces, allowing seamless integration of different data types.</li> <li> <strong>Importance of Domain Adaptation</strong>: High-performing models adapt well to various document types and domains, effectively handling specific jargon and layouts.</li> <li> <strong>Interpretability Matters</strong>: Interpretability is essential for user trust and compliance, with models like ColPali providing transparent retrieval processes through attention-based heatmaps.</li> <li> <strong>Innovative Evaluation is Crucial</strong>: New benchmarking strategies and evaluation metrics are vital for assessing the strengths of multimodal models in complex retrieval tasks.</li> </ol> <p><em>“The modality gap isn’t just being bridged—it’s being obliterated.”</em></p> <p><em>“The future is bright, and it’s multimodal.”</em></p> <p><strong>Nikhil Reddy</strong> is a researcher at Mila, Quebec, specializing in machine learning, computer vision, and information retrieval systems.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-multimodal-vector-search.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
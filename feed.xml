<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nikhilreddy3888.github.io/2025/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nikhilreddy3888.github.io/2025/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-14T06:12:11+08:00</updated><id>https://nikhilreddy3888.github.io/2025/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">Sample Blog Post</title><link href="https://nikhilreddy3888.github.io/2025/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://nikhilreddy3888.github.io/2025/blog/distill-example</id><content type="html" xml:base="https://nikhilreddy3888.github.io/2025/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> Command Not Found: mmdc </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://nikhilreddy3888.github.io/2025/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://nikhilreddy3888.github.io/2025/blog/distill-example2</id><content type="html" xml:base="https://nikhilreddy3888.github.io/2025/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/2025/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> Command Not Found: mmdc </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings</title><link href="https://nikhilreddy3888.github.io/2025/blog/multimodal-vector-search/" rel="alternate" type="text/html" title="Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings"/><published>2024-11-20T00:00:00+08:00</published><updated>2024-11-20T00:00:00+08:00</updated><id>https://nikhilreddy3888.github.io/2025/blog/multimodal-vector-search</id><content type="html" xml:base="https://nikhilreddy3888.github.io/2025/blog/multimodal-vector-search/"><![CDATA[<p>In today’s data-driven world, the volume and complexity of information have grown exponentially. Documents are no longer confined to plain text; they now encompass a rich blend of images, charts, tables, and intricate layouts. This evolution presents a significant challenge: <strong>How do we effectively retrieve and analyze information from such complex, multimodal documents?</strong> <br/> Traditional retrieval systems, primarily designed for text-only data, often falter when faced with this complexity. They struggle to extract and interpret the valuable information embedded within various modalities. This limitation hampers our ability to harness the full potential of the data at our disposal. <br/> Enter <strong>Multimodal Embeddings</strong>—an innovative approach that leverages both textual and visual data to revolutionize document retrieval. By bridging the <strong>modality gap</strong> between different data types, MVS(MultiModal Vector Search) promises to make information retrieval more accurate and efficient than ever before. <br/> In this blog post, we’ll delve into:</p> <ul> <li>The unique challenges that modern, complex documents pose for retrieval tasks.</li> <li>The limitations of traditional text-only and vision-only models.</li> <li>Understanding the <strong>modality gap</strong> and its impact on multimodal retrieval.</li> <li>Exploring cutting-edge multimodal models like <strong>VISTA</strong> <d-cite key="Zhou2024VISTA"></d-cite> and <strong>ColPali</strong> <d-cite key="Faysse2024ColPali"></d-cite>.</li> <li>Investigating the interpretability, with empirical results showcasing model performance.</li> <li>Novel benchmarking strategies designed to evaluate these advanced models effectively.</li> </ul> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/DSE.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: Integration of text and visual modalities in document retrieval. <d-cite key="ma2024unifyingmultimodalretrievaldocument"></d-cite> </div> <p>With the rise of sophisticated document formats that integrate text, images, and complex layouts has rendered traditional text-based retrieval systems inadequate. The richness of multimodal documents requires systems that can understand and process multiple data types simultaneously. The <strong>modality gap</strong>—the disconnect between different types of data representations—poses a significant hurdle in achieving effective retrieval<d-cite key="Liang2022MindGap"></d-cite>.</p> <p>To bridge this gap, advanced multimodal systems are essential. By aligning and embedding various data types, these systems not only interpret complex documents but also open doors to more powerful, nuanced information retrieval across diverse formats.</p> <h2 id="challenges-in-document-retrieval">Challenges in Document Retrieval</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/main-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/main.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: Illustration of the increasing levels of detail in images, from basic visuals to more information-dense representations. </div> <h3 id="modality-specific-challenges">Modality-Specific Challenges</h3> <p>Retrieving information from today’s complex documents is a multifaceted problem that requires a nuanced approach. Documents now often contain:</p> <ul> <li><strong>Text</strong>: Dense paragraphs, bullet points, and annotations.</li> <li><strong>Images</strong>: Photographs, diagrams, and illustrations.</li> <li><strong>Tables and Charts</strong>: Structured data representations.</li> <li><strong>Complex Layouts</strong>: Multi-column formats, sidebars, and embedded multimedia.</li> </ul> <p>Each modality presents unique technical obstacles:</p> <ol> <li><strong>Textual Data</strong>: <ul> <li><strong>Language Ambiguity</strong>: Synonyms, homonyms, and context-dependent meanings.</li> <li><strong>Multilingual Content</strong>: Documents may contain multiple languages or dialects.</li> </ul> </li> <li><strong>Visual Data</strong>: <ul> <li><strong>Image Quality</strong>: Low-resolution images can hinder recognition.</li> <li><strong>Complex Visuals</strong>: Diagrams and charts may contain dense information that’s hard to parse.</li> </ul> </li> <li><strong>Structural Layout</strong>: <ul> <li><strong>Non-linear Reading Paths</strong>: Multi-column texts and inserts can confuse linear text processors.</li> <li><strong>Embedded Elements</strong>: Images and tables interwoven with text complicate parsing.</li> </ul> </li> </ol> <p>Addressing these challenges requires advanced retrieval systems that are capable of seamlessly integrating and processing each modality, making it possible to extract meaningful insights from even the most complex document formats.</p> <h3 id="modality-gap">Modality Gap</h3> <p>The <strong>modality gap</strong> refers to the disconnect between text and image embeddings in multimodal models. Despite shared semantic objectives, these embeddings often occupy distinct regions in the space, making it difficult for models to relate information across modalities<d-cite key="Liang2022MindGap"></d-cite>.</p> <h4 id="causes-of-the-gap">Causes of the Gap</h4> <ol> <li><strong>Separate Embedding Spaces</strong>: Text and images are encoded differently, forming distinct clusters.</li> <li><strong>Contrastive Learning Bias</strong>: Training inadvertently emphasizes modality-specific features.</li> <li><strong>Initialization Bias</strong>: Pretrained encoders begin with cone-shaped distributions, reinforcing separation.</li> </ol> <h4 id="evidence-from-flickr8k-dataset">Evidence from Flickr8k Dataset</h4> <p>Using 1,000 text-image pairs from the Flickr8k test set, each pair consists of a single image and five caption texts, all describing the same image. Embeddings were generated for both the texts and the image, and cosine similarity was calculated and visualized.</p> <ul> <li><strong>Cosine Similarity</strong>: Text-text pairs had higher similarity than image-text pairs, despite semantic overlap.</li> <li><strong>Visualization</strong>: Embeddings from the Flickr8k dataset showed clustering into distinct text and image regions.</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/Modality.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: Visualization of the modality gap between text and image embeddings. </div> <h4 id="implications">Implications</h4> <ul> <li><strong>Retrieval Challenges</strong>: Models may retrieve documents that match in one modality but are irrelevant in another.</li> <li><strong>Increased Complexity</strong>: Bridging this gap demands advanced architectures and computational resources.</li> </ul> <p>For more details on the Modality Gap, refer to <a href="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/">this article</a>.</p> <h2 id="limitations-of-traditional-models">Limitations of Traditional Models</h2> <h3 id="text-based-models">Text-Based Models</h3> <p>Text-based retrieval models—leveraging techniques like TF-IDF, BM25, and transformer-based embeddings such as BERT—have been the cornerstone of information retrieval. They excel at understanding and retrieving information when text is the primary medium. <d-cite key="devlin2019bertpretrainingdeepbidirectional"></d-cite> <d-cite key="reimers2019sentencebertsentenceembeddingsusing"></d-cite></p> <p><strong>Limitations:</strong></p> <ul> <li><strong>Blind to Visual Content</strong>: Unable to interpret images, charts, or layouts.</li> <li><strong>Ignoring Spatial Relationships</strong>: Can’t understand the importance of where text appears in a document.</li> <li><strong>Struggling with Non-Linear Layouts</strong>: Fail to process documents with complex formatting.</li> </ul> <h3 id="vision-based-models">Vision-Based Models</h3> <p>Vision-based retrieval models, utilizing architectures like convolutional neural networks (CNNs) or vision transformers (e.g., ViT, Swin Transformer), extract features from visual content, focusing on images, diagrams, and spatial layouts. <d-cite key="he2015deepresiduallearningimage"></d-cite> <d-cite key="liu2021swintransformerhierarchicalvision"></d-cite></p> <p><strong>Limitations:</strong></p> <ul> <li><strong>Text Interpretation</strong>: Struggle with reading or understanding embedded text within images.</li> <li><strong>Fine-Grained Details</strong>: May overlook small fonts or intricate details in complex diagrams.</li> <li><strong>Semantic Gap</strong>: Lack understanding of the textual semantics associated with visual elements.</li> </ul> <h2 id="advanced-evaluation-metrics-for-ranking">Advanced Evaluation Metrics for Ranking</h2> <p>Evaluating retrieval systems, especially those handling multimodal data, demands metrics that account for both relevance and ranking position. <d-cite key="arabzadeh2024comparisonmethodsevaluatinggenerative"></d-cite> <d-cite key="jadon2024comprehensivesurveyevaluationtechniques"></d-cite></p> <h3 id="mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</h3> <p><strong>Definition:</strong> The MRR measures how quickly a system retrieves the first relevant document.</p> \[\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}\] <p>Where:</p> <ul> <li>\(( Q )\) is the number of queries.</li> <li>\(( {rank}_i )\) is the rank position of the first relevant document for the ( i )-th query.</li> </ul> <p><strong>Importance:</strong></p> <ul> <li><strong>User Satisfaction</strong>: Higher MRR indicates users find relevant documents sooner.</li> <li><strong>System Efficiency</strong>: Reflects the system’s ability to prioritize relevant results.</li> </ul> <h3 id="normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (nDCG)</h3> <p><strong>Definition:</strong> nDCG evaluates ranking quality by considering the position of relevant documents and assigning higher importance to top-ranked results.</p> \[\text{nDCG}_p = \frac{1}{\text{IDCG}_p} \sum_{i=1}^p \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}\] <p>Where:</p> <ul> <li>\(( p )\) is the number of results considered.</li> <li>\(( {rel}_i )\) is the relevance score of the result at position ( i ).</li> <li>\(( {IDCG}_p )\) is the ideal DCG up to position ( p ).</li> </ul> <p><strong>Importance:</strong></p> <ul> <li><strong>Relevance and Rank</strong>: Balances both factors to provide a holistic evaluation.</li> <li><strong>Comparison Across Queries</strong>: Normalization allows for fair comparison between different queries.</li> </ul> <h3 id="mean-average-precision-map">Mean Average Precision (MAP)</h3> <p><strong>Definition:</strong> MAP computes the mean of average precision scores across all queries.</p> \[\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{n_q} \sum_{k=1}^{n_q} \text{Precision}(k) \times \text{rel}(k)\] <p>Where:</p> <ul> <li>\(( {n}_q )\) is the number of retrieved documents for query ( q ).</li> <li>\(( {Precision}(k) )\) is the precision at cutoff ( k ).</li> <li>\(( {rel}(k) )\) is a binary indicator of relevance at position ( k ).</li> </ul> <p><strong>Importance:</strong></p> <ul> <li><strong>Comprehensive Evaluation</strong>: Accounts for all relevant documents, not just the first.</li> <li><strong>Balanced Metric</strong>: Reflects both precision and recall across the ranking.</li> </ul> <h2 id="recent-advancements-in-multimodal-retrieval-models">Recent Advancements in Multimodal Retrieval Models</h2> <h3 id="vista-vision-augmented-text-embeddings">VISTA: Vision-Augmented Text Embeddings</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/vista-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/vista.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: VISTA's architecture integrates visual tokens into text embeddings.<d-cite key="Zhou2024VISTA"></d-cite> </div> <p><strong>Overview:</strong></p> <p>VISTA (Visualized Text Embedding For Universal Multi-Modal Retrieval) is a model that aims to enhance text embeddings with visual information, effectively bridging the modality gap.</p> <p><strong>How VISTA Works:</strong></p> <ul> <li><strong>Visual Token Embedding</strong>: Introduces visual tokens derived from document images into the text embedding space.</li> <li><strong>Extension of Text Encoders</strong>: Enhances pre-trained text models (like BERT) by adding a visual component.</li> <li><strong>Multi-Stage Training</strong>: <ul> <li><strong>Alignment Phase</strong>: Aligns visual tokens with textual tokens using a large corpus.</li> <li><strong>Fine-Tuning Phase</strong>: Trains on composed image-text data for multimodal representation capability.</li> </ul> </li> </ul> <p><strong>Strengths:</strong></p> <ul> <li><strong>Improved Retrieval Accuracy</strong>: Enhances understanding of documents with embedded images.</li> <li><strong>Compatibility</strong>: Works with existing text encoders.</li> <li><strong>Efficiency</strong>: Avoids the need for extensive retraining.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li><strong>Dependency on Visual Quality</strong>: Performance may degrade with low-quality images.</li> <li><strong>Complexity in Token Integration</strong>: Requires careful balancing to prevent one modality from dominating.</li> </ul> <h3 id="colpali-efficient-document-retrieval-with-vision-language-models">ColPali: Efficient Document Retrieval with Vision Language Models</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/colpali.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 4: ColPali's unified embedding space for text and images.<d-cite key="Faysse2024ColPali"></d-cite> </div> <p><strong>Overview:</strong></p> <p>ColPali is a state-of-the-art multimodal retrieval model that leverages Vision-Language Models (VLMs) to create a unified embedding space for text and images.</p> <p><strong>How ColPali Works:</strong></p> <ul> <li><strong>Vision-Language Integration</strong>: Uses a dual-encoder architecture with separate encoders for text and images.</li> <li><strong>Late Interaction Mechanism</strong>: Retains individual token embeddings for richer interactions.</li> <li><strong>Elimination of OCR Bottleneck</strong>: Processes images directly, capturing both textual and visual information without OCR.</li> </ul> <p><strong>Strengths:</strong></p> <ul> <li><strong>Effective Modality Bridging</strong>: Creates a shared space for all modalities.</li> <li><strong>Enhanced Retrieval Performance</strong>: Excels in retrieving documents with complex layouts.</li> <li><strong>Interpretability</strong>: Allows for analyzing which tokens contribute most to the retrieval score.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li><strong>Computational Demands</strong>: Training VLMs is resource-intensive.</li> <li><strong>Data Requirements</strong>: Requires large amounts of multimodal data.</li> <li><strong>Potential Overfitting</strong>: May overfit to specific layouts or styles.</li> </ul> <h2 id="interpretability-of-advanced-models-in-documents">Interpretability of Advanced Models in Documents</h2> <h3 id="importance-of-interpretability">Importance of Interpretability</h3> <p>Understanding how a model like ColPali makes retrieval decisions is crucial, particularly in sensitive domains like finance, where accuracy and accountability are paramount.</p> <ul> <li><strong>Transparency</strong>: By identifying the specific document regions influencing retrieval, ColPali provides clear insights into its decision-making process, fostering user trust.</li> <li><strong>Debugging</strong>: The ability to pinpoint errors allows developers to refine and optimize the model effectively.</li> </ul> <h3 id="generating-heatmaps-using-attention-mechanisms">Generating Heatmaps Using Attention Mechanisms</h3> <p>Models leverages attention mechanisms to produce interpretable heatmaps that highlight the most relevant regions of a document in response to a query. By computing attention scores between query tokens and image patches, the model identifies which parts of the document image are most influential in the retrieval process.</p> <p><strong>How Heatmaps are Generated:</strong></p> <ol> <li><strong>Image Patches and Query Tokens</strong>: The document image is divided into fixed-size patches, and the query is broken down into individual tokens.</li> <li><strong>Embedding Computation</strong>: The model computes embeddings for both the image patches and the query tokens using its vision and language encoders.</li> <li><strong>Attention Score Calculation</strong>: Attention scores are calculated by taking the dot product between each query token embedding and each image patch embedding.</li> <li><strong>Normalization and Mapping</strong>: These scores are normalized to highlight the most significant interactions and are mapped back onto the spatial layout of the image patches.</li> <li><strong>Visualization</strong>: The normalized attention scores are overlaid onto the original document image, creating heatmaps that visually represent the areas of focus for each query token.</li> </ol> <p>This approach provides a transparent way to understand which parts of the document are most relevant to the query, combining both textual and visual information.</p> <h3 id="heatmap-insights-case-studies">Heatmap Insights: Case Studies</h3> <p><strong>Example 1: Alibaba’s 10-K Report</strong></p> <ul> <li><strong>Query</strong>: “What are the artificial intelligence tools being developed?”</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/AI_heat_map.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 5: Heatmaps overlaid on Alibaba's 10-K report highlighting "artificial" and "intelligence". </div> <ul> <li><strong>Results</strong>: ColPali retrieved relevant sections emphasizing AI tools and infrastructure, such as “AI-driven infrastructure” and “intelligent monitoring.”</li> <li><strong>Explanation</strong>: The heatmaps show that the model’s attention is concentrated on areas containing the terms “artificial” and “intelligence,” as well as related phrases like “AI” and “intelligent.” This indicates that ColPali effectively aligns query tokens with corresponding regions in the document image, highlighting its ability to interpret both textual content and visual context.</li> </ul> <p><strong>Example 2: Royal Bank of Canada’s Annual Report</strong></p> <ul> <li><strong>Query</strong>: “Describe the elements of stock-based compensation.”</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map-1400.webp"/> <img src="/2025/assets/img/2025-04-28-multimodal-vector-search/stock_heat_map.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 6: Heatmaps highlighting relevant terms "stock" and "compensation" in RBC's annual report. </div> <ul> <li><strong>Results</strong>: Pages discussing “stock-based compensation plans” and related financial methodologies were retrieved.</li> <li><strong>Explanation</strong>: The heatmaps reveal that the model focuses on sections containing the words “stock,” “compensation,” and related terms like “share-based” and “shareholders.” Even when the text appears in smaller fonts or footnotes, the model effectively identifies these areas. This demonstrates ColPali’s capability to extract precise financial details from complex document layouts by leveraging attention mechanisms.</li> </ul> <p>These examples showcase how ColPali’s use of attention-based heatmaps provides interpretable insights into its retrieval decisions, highlighting the relevance between query tokens and document regions.</p> <p>For more details on the interpretability and the underlying code, refer to <a href="https://medium.com/@hlealpablo/interpretability-of-colpali-in-financial-documents-5a2dcdeeba3a">this article</a>.</p> <h2 id="innovative-evaluation-methods-simple-yet-effective">Innovative Evaluation Methods (Simple yet Effective)</h2> <h3 id="feature-search-experiments">Feature Search Experiments</h3> <p><strong>Objective:</strong></p> <p>Assess how effectively models generate embeddings that capture similarities between documents across modalities.</p> <p><strong>Methodology:</strong></p> <ol> <li><strong>Embedding Extraction</strong>: Used models like <strong>BGE-M3</strong>, <strong>VISTA</strong>, <strong>ColPali</strong>, and <strong>ColQwen</strong>.</li> <li><strong>Similarity Computation</strong>: Calculated cosine similarity between document embeddings.</li> <li><strong>Evaluation Metrics</strong>: Employed metrics such as <strong>Precision@K</strong>, <strong>Recall@K</strong>, <strong>MRR</strong>, <strong>MAP</strong>, and <strong>nDCG</strong>.</li> </ol> <p><strong>Results:</strong></p> <ul> <li><strong>Text-Based Models</strong>: High precision when text is the differentiator but struggled with visual nuances.</li> <li><strong>Multimodal Models</strong>: Outperformed in tasks where visual context is important, effectively capturing both textual and visual similarities.</li> </ul> <h3 id="feature-similarity-experiments">Feature Similarity Experiments</h3> <p><strong>Objective:</strong></p> <p>Evaluate the usefulness of embeddings in differentiating between various document types.</p> <p><strong>Methodology:</strong></p> <ol> <li><strong>Prototype Vectors Generation</strong>: Created prototype vectors for each document category (e.g., invoices, contracts, reports) representing the “center” of the embedding space for that class.</li> <li><strong>Similarity with Cluster Centers</strong>: Compared new document embeddings to these prototype vectors using cosine similarity, classifying each document into the category with the highest similarity score.</li> <li><strong>Evaluation Metrics</strong>: Used metrics such as <strong>Accuracy</strong> and <strong>Recall</strong> to assess how well the embeddings captured similarities within categories and differences between them.</li> </ol> <p><strong>Findings:</strong></p> <ul> <li><strong>Text-Based Models</strong>: Effectively categorized documents with distinct textual content.</li> <li><strong>Multimodal Models</strong>: Performed better when visual and layout information were crucial for classification, highlighting their ability to capture complex document features.</li> </ul> <h3 id="interactive-embedding-visualization">Interactive Embedding Visualization</h3> <p>Below is an interactive t-SNE plot showing document embeddings colored by category. This visualization provides insights into how different models (e.g., VISTA, ColPali, SigLIP) represent document categories (e.g., forms, invoices, identity documents). The embeddings are displayed in a two-dimensional space, highlighting clustering patterns and category separations.</p> <div class="row mt-3"> <iframe src="/2025/assets/html/2025-04-28-multimodal-vector-search/final_combined_embeddings.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Figure 7: Interactive t-SNE plot of document embeddings colored by category. </div> <p>This additional visualization allows for a deeper understanding of how well multimodal models differentiate between document types, bridging the modality gap through effective embedding representations.</p> <h2 id="experimental-evaluations">Experimental Evaluations</h2> <p>To evaluate the performance of multimodal retrieval models, we utilized the <strong>ViDoRe Benchmark</strong>, a comprehensive collection designed for assessing document retrieval using visual features. This benchmark includes datasets formatted in a Question-Answering (QA) style to simulate realistic retrieval scenarios.</p> <p>These datasets encompass a wide range of document types, including financial reports, legal documents, academic papers, manuals, and healthcare records. Each dataset presents unique challenges due to varying content complexity, layouts, and modality combinations.</p> <h3 id="performance-comparison">Performance Comparison:</h3> <p>The table below summarizes the performance of various models across the ViDoRe datasets, measured by the Normalized Discounted Cumulative Gain at rank 5 (NDCG@5).</p> <table> <thead> <tr> <th><strong>Model Name</strong></th> <th><strong>Average</strong></th> <th><strong>TAT-DQA</strong></th> <th><strong>Shift Project</strong></th> <th><strong>Artificial Intelligence</strong></th> <th><strong>Government Reports</strong></th> <th><strong>ArxivQA</strong></th> <th><strong>DocVQA</strong></th> <th><strong>Healthcare Industry</strong></th> <th><strong>InfoVQA</strong></th> <th><strong>Energy</strong></th> <th><strong>TabFQuad</strong></th> </tr> </thead> <tbody> <tr> <td><strong>ColQwen2</strong></td> <td><strong>89.3</strong></td> <td>81.4</td> <td>90.7</td> <td>99.4</td> <td>96.3</td> <td>88.1</td> <td>60.6</td> <td>98.1</td> <td>92.6</td> <td>95.9</td> <td>89.5</td> </tr> <tr> <td><strong>ColPali</strong><d-cite key="Faysse2024ColPali"></d-cite></td> <td>81.3</td> <td>65.8</td> <td>73.2</td> <td>96.2</td> <td>92.7</td> <td>79.1</td> <td>54.4</td> <td>94.4</td> <td>81.8</td> <td>91.0</td> <td>83.9</td> </tr> <tr> <td><strong>VISTA</strong>*<d-cite key="Zhou2024VISTA"></d-cite></td> <td>70.8</td> <td>56.9</td> <td>78.6</td> <td>86.8</td> <td>89.3</td> <td>39.4</td> <td>32.2</td> <td>91.1</td> <td>75.0</td> <td>87.7</td> <td>71.2</td> </tr> <tr> <td><strong>E5-Large</strong>*<d-cite key="Wang2022E5"></d-cite></td> <td>65.0</td> <td>51.0</td> <td>61.1</td> <td>87.9</td> <td>84.8</td> <td>34.0</td> <td>27.8</td> <td>85.5</td> <td>63.5</td> <td>81.6</td> <td>73.1</td> </tr> <tr> <td><strong>BGE-M3</strong>*<d-cite key="Xiao2023BGE"></d-cite></td> <td>67.0</td> <td>43.8</td> <td>73.1</td> <td>88.8</td> <td>80.4</td> <td>35.7</td> <td>32.9</td> <td>91.3</td> <td>71.9</td> <td>83.3</td> <td>69.1</td> </tr> <tr> <td><strong>BM25</strong></td> <td>65.5</td> <td>62.7</td> <td>64.3</td> <td>92.8</td> <td>83.9</td> <td>31.6</td> <td>36.8</td> <td>87.2</td> <td>62.9</td> <td>85.9</td> <td>46.5</td> </tr> <tr> <td><strong>SigLIP</strong><d-cite key="zhai2023sigmoid"></d-cite></td> <td>51.4</td> <td>26.2</td> <td>18.7</td> <td>62.5</td> <td>66.1</td> <td>43.2</td> <td>30.3</td> <td>79.1</td> <td>64.1</td> <td>65.7</td> <td>58.1</td> </tr> <tr> <td><strong>Jina-CLIP</strong><d-cite key="koukounas2024jina"></d-cite></td> <td>17.7</td> <td>3.3</td> <td>3.8</td> <td>15.2</td> <td>21.4</td> <td>25.4</td> <td>11.9</td> <td>20.8</td> <td>35.5</td> <td>19.7</td> <td>20.2</td> </tr> </tbody> </table> <p><strong>Note</strong>: Models marked with <code class="language-plaintext highlighter-rouge">*</code> (e.g., VISTA, E5-Large, BGE-M3) have been re-evaluated on the ViDoRe Benchmark.</p> <p><strong>Observations:</strong></p> <ul> <li> <p><strong>Top Performer:</strong> The <strong>ColQwen2</strong> model achieved the highest average NDCG@5 score of <strong>89.3</strong>, outperforming other models across most datasets.</p> </li> <li> <p><strong>Strong Multimodal Performance:</strong> <strong>ColQwen2</strong>, <strong>VISTA</strong> and <strong>ColPali</strong> performed well across multiple datasets, showing robust results on specific domain datasets like <strong>Healthcare Industry</strong> and <strong>Artificial Intelligence</strong>.</p> </li> <li> <p><strong>Text vs. Vision Models:</strong> Traditional text-based models such as <strong>BGE-M3</strong> and <strong>BM25</strong> showed competitive results on certain datasets but generally lagged behind multimodal models. Vision-only models like <strong>SigLIP</strong> and <strong>Jina-CLIP</strong> struggled significantly, especially on text-heavy datasets.</p> </li> <li> <p><strong>Dataset Variability:</strong> The performance variance across datasets indicates that some models are better suited for specific domains. For instance, <strong>ColQwen2</strong> excelled in <strong>Government Reports</strong> (96.3) and <strong>Energy</strong> (95.9), suggesting robustness in handling domain-specific jargon and layouts.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The shift from unimodal to multimodal approaches in document retrieval is revolutionizing how we access complex information. Models like <strong>VISTA</strong>, <strong>ColPali</strong>, and <strong>ColQwen2</strong> not only bridge the modality gap but also set new benchmarks for performance across diverse and complex datasets.</p> <h3 id="key-takeaways">Key Takeaways:</h3> <ol> <li><strong>Multimodal Models Excel</strong>: Combining textual and visual features significantly improves retrieval accuracy, especially for documents with complex layouts.</li> <li><strong>Advanced Models Address Modality Challenges</strong>: Models like ColPali and ColQwen2 create unified embedding spaces, allowing seamless integration of different data types.</li> <li><strong>Importance of Domain Adaptation</strong>: High-performing models adapt well to various document types and domains, effectively handling specific jargon and layouts.</li> <li><strong>Interpretability Matters</strong>: Interpretability is essential for user trust and compliance, with models like ColPali providing transparent retrieval processes through attention-based heatmaps.</li> <li><strong>Innovative Evaluation is Crucial</strong>: New benchmarking strategies and evaluation metrics are vital for assessing the strengths of multimodal models in complex retrieval tasks.</li> </ol> <p><em>“The modality gap isn’t just being bridged—it’s being obliterated.”</em></p> <p><em>“The future is bright, and it’s multimodal.”</em></p> <p><strong>Nikhil Reddy</strong> is a researcher at Mila, Quebec, specializing in machine learning, computer vision, and information retrieval systems.</p>]]></content><author><name>Nikhil Reddy</name></author><summary type="html"><![CDATA[In an era where documents seamlessly blend rich textual and visual content, traditional retrieval systems often fall short. This post explores how Multimodal Embeddings bridge the modality gap, introducing advanced models such as VISTA and ColPali, along with innovative evaluation methods, enhanced model interpretability, and an analysis of their performance.]]></summary></entry></feed>